{"pages":[],"posts":[{"title":"CSE168 Final Project Proposal","text":"Rendering Equation $$L_r(x, \\omega_o) = L_e(x, \\omega_o) +\\int_{\\Omega}f(x, \\omega_i, \\omega_o)L_i(x, \\omega_i)(n \\cdot \\omega_i)\\ d\\omega_i$$ IntroductionFor final project, I am going to implement algorithms related to transparent materials. In particular, I would like to create images with realistic refractions and caustic effects. Below, I will discuss what I have done so far, and what I plan to implement for this project. Refactoring the CodeThe first thing I did is refactoring the code. In submission of project 4, my PathTracer.cu almost reached 600 lines. So, before writing any new code, I first adjusted the structure of my path tracer, and extract codes related to BRDF, PDF and importance sampling into a new header file BSDF.h. In the end, I reduced PathTracer.cu to around 300 lines, and new BSDF.h has a clear structure of functions evaluating and sampling each material. Loading ModelsPrevious projects support spheres and meshes made up of triangles. However, information of triangles need to be provided in the scene file in a unique way. To have a more flexible control of the scene, I implemented functions to load .obj files. This is achieved by a single command, and other components of the scene can be specified the old way. Now, I am able to compose new scenes with complex meshes easily. 12#sphere 0 0 0 1obj Scenes/final/bunny.obj For example, with the minor change I show above, I can load the Stanford bunny into the scene: Sphere Bunny With similar modifications, I also replaced spheres in GGX scene by Stanford bunny. This simple model loading command is compatible with all old material parameters, and position transformation commands. New models can also be sampled correctly by old importance sampling methods. The library I used to load .obj files is libigl. This powerful mesh processing library also provides interesting functions like model decimation: Bunny with 5,000 Faces Bunny with 500 Faces Simple Glass MaterialNext, I implemented a simple glass material which only considers refractions and total inner reflections. New material can be specified by brdf glass. The problem brought by this new material is that we can no longer do NEE when our sampling ray is blocked by the glass object. Because, after the sampling ray get refracted, it would no longer point to the region of the light source we initially sampled. One solution is to completely turn off NEE. But it requires a lot more samples for decent results. 256 Samples without NEE 2048 Samples without NEE A trick I done is to only turn off NEE for a single ray if there is a glass object between the light source and the sampled region. This way, most regions are still rendered with NEE and noise free. Only the shadows and caustics under glass objects are noisy, but they are still correct. Although the result looks weird, it provides a quick way to debug, as it requires less samples. 256 Samples with Partial NEE 2048 Samples without NEE Final Project ProposalFor the final project, the first thing I will implement is Fresnel effect for the glass material. This should be straightforward by generating a random number and check it against Schlick’s approximation. With Fresnel effect, the glass objects will have reflections as well. This will provide more realistic appearance. Next, I will implement photon mapping technique. Currently, caustic effects require huge number of samples. The partial NEE method can present clean surfaces with less samples, but the shadows and caustics are noisy and unnatural. With photon mapping, I hope to render realistic caustics and clean images within a small amount of time. Lastly, I will model a larger scene with multiple light sources and more interesting glass objects, as well as GGX and Phong objects. Together, they will fully illustrate the beautiful and realist reflections, refractions and caustics rendered by my photon mapping path tracer. If time permits, I will also implement microfacet model for refractions, so glass object can have rough surfaces.","link":"/TonyZYT2000.github.io.archive/en/CSE168-Final-Project-Proposal/"},{"title":"CSE168 Final Project","text":"IntroductionIn this final project, I implement photon mapping techniques and combine it with the path tracer. The rendering process takes two passes. In the first pass, photons are shot from light sources toward glass objects only, and cached on the first non-glass surface they hit. In the second pass, rays are shot from the camera, and on each intersection, we compute its radiance by adding up NEE and caustics calculated from the photon map. Then, we do the recursive path tracing and terminate based on Russian roulette. During this project, I rely heavily on Professor Jensen’s paper and course notes on photon mapping. Other online tutorials on photon mapping, reflection and refraction are also consulted during my implementation. All the materials I referred are provided at the end. I also want to say thanks to professor Ramamoorthi, and our TAs Kuznetsov and Shafiei. They gave me many advice throughout the project. Below, I will document my implementations in detail. Glass BSDFThe first component of the project is a correct glass BSDF that describes how rays reflect and refract with glass surfaces. Whether to reflect or refract is determined randomly based on current Fresnel term, which is approximated by Schlick approximation:$$R(\\theta) = R_0 + (1 - R_0) \\cdot (1 - cos\\theta)^5, R_0 = \\left(\\frac{n_1 - n_2}{n_1 + n_2}\\right)^2$$Here, $n_1, n_2$ are refraction indices of two materials. These terms are used in refraction calculation as well. I choose $1.5$ for the glass, and $1$ for the air. Whether the ray is shooting from air to glass, or from glass to air is handled by checking against the normal vector. When the random number is higher than Fresnel term, rays get refracted. When the random number is lower than Fresnel term, rays get reflected. Also, special case of total inner reflections would also happen in refraction case, and is calculated accordingly. Directions of reflection and refraction are computed following mirror reflection equation and Snell’s equation for refraction. Corresponding BSDFs and PDFs are computed as: $$f_{TIR} = \\frac{1}{\\mid cos\\theta_i \\mid}, \\ pdf_{TIR} = 1$$ $$f_{reflect} = \\frac{R(\\theta_i)}{\\mid cos\\theta_i \\mid}, pdf_{reflect} = R(\\theta_i)$$ $$f_{refract} = \\frac{1 - R(\\theta_i)}{\\mid cos\\theta_i \\mid}, pdf_{refract} = 1 - R(\\theta_i)$$ Lastly, to handle NEE correctly, I treat glass object as nontransparent, so there would be just shadows. Then, I turn off NEE for glass object. Instead, I let rays that reflect on glass object also add the kEmission term on next non-glass intersection. This would reproduce the glossy effect on glass materials accurately. Other materials are still handled by NEE, and kEmission is added only in first intersection. Note total inner reflections are considered as reflection in above strategy as well. The highlight on bunny’s ears not only comes from upper surface reflection, but also total inner reflection from the lower surface. Glass Sphere without Caustics Glass Bunny without Caustics Photon CachingThe next step would be shooting photons, tracing photons and caching them on the surface. Shooting photons is implemented as a new OptiX ray generation device written in PhotonMapper.cu. Tracing photons is handled by adding another ray type and corresponding closeHit program defined in PhotonTracer.cu. Hundreds of photons are generated from each square light source. Each photon’s initial position is sampled uniformly on the square light source, and direction is sampled from a cosine distribution. The initial intensity is then given by: $I_p = \\frac{I_l \\cdot A_l \\cdot \\cos\\theta}{\\frac{\\cos\\theta}{\\pi}} = I_l \\cdot A_l \\cdot \\pi$, where $I_l$ is the intensity of the light source and $A_l$ is the area of the light source. During the photon shooting stage, the closeHit program check if it hits a glass object, and only do recursive trace if the photon hit a glass surface first. This essentially is an additional rejection sampling, and the count of actual trails is recorded in a buffer that would be divided in the intensity of the photon later. On each intersection with glass surface, the next direction is calculated by calling BSDF sampling function for glass object. The intensity is then timed by the corresponding BSDF. Once the photon gets out of glass object and hit a diffuse surface, its position, intensity and direction are stored in a buffer holding all photons. Below are some virtualizations of how photons are cached on diffuse surfaces. The integrator turn radiance to $1$ if a pixel is close enough to a photon. This is a helpful debug function I used frequently. Glass Sphere with Cached Photons Glass Bunny with Cached Photons Path Tracing with Photon MappingThe last step would be path tracing and estimate caustics by photon mapping. These are implemented in PMPathTracer.cu as a new integrator, and can be used in scene file by including integrator photonmapping. The photon virtualization function I mentioned above, and the calculation of caustics from photon map are both written in this new path tracer. Caustics are calculated based on density estimation of photons. Because of limited time and the limitation in OptiX, I stored photons in a naïve linear buffer. On each intersection, nearest photons are searched linearly, and inserted into a binary heap where distance is the priority. This would make the root element having largest distance. So, once the heap is full, we replace the root if we find a closer photon, and then sort to heap order. This binary heap was initially implemented using primitive array defined in heap.h. But, it turns out when the size get larger, the memory on GPU got messed up. Thus, the final version is implemented using a huge buffer of size $width \\cdot height \\cdot heapSize$. Each ray for each pixel will have their own space to maintain the heap, and would not interfere with each other. With a working heap that can store any number of nearest photons, the caustic estimation is given by:$$\\sum_{p=1}^N f(x, \\omega_o, \\omega_{i,p}) \\cdot\\frac{\\Delta\\Phi_P(x, \\omega_{i,p})}{\\pi r^2}$$ $f(x, \\omega_o, \\omega_{i,p})$ is the BRDF of the intersecting position $x$ given viewing direction and photon direction. $\\Delta \\Phi_p(x, \\omega_{i,p})$ is the intensity of the photon arriving at $x$ from direction $\\omega_{i,p}$. $r$ is the largest distance of $N$ nearest photons. Then, since further photons should have less contribution to this estimation, I adopt the cone filter which assign a weight $w_p = 1 - \\frac{d_p}{r}$ for each photon. So the final estimation equation is given by:$$\\frac{1}{\\pi r^2} \\cdot \\sum_{p=1}^N f(x, \\omega_o, \\omega_{i,p}) \\cdot\\Delta\\Phi_P(x, \\omega_{i,p}) \\cdot \\left(1 - \\frac{d_p}{r}\\right)$$Then, we can get following results. They are rendered by the photon mapping path tracer I described above, with $64$ samples each pixel, $200$ photons from light source, and $N = 20$ in caustic estimation. Glass Sphere with Caustics Glass Bunny with Caustics Here is a comparison of a larger scene where caustics effect is subtle. Notice how the ground around dragon’s feet are lit with caustics calculated from photon map. Glass Dragon without Caustics Glass Dragon with Cached Photons Glass Dragon with Caustics FlawCaustics seems nice in examples I present above, but there is a flaw. Because the caustics are estimated from density distribution, they have a blur boundary and smoother looking comparing with real caustics. Maybe a better weighting function could help. Also, increasing the number of photons and adjust how many photons we collect for estimation could improve the result. But, I don’t really find a perfect set of parameters to reproduce actual details in caustics. Here are some comparisons between path tracing results and photon mapping results: Path Tracing with 2048 Samples Photon Mapping with 64 Samples SummaryAll in all, I implement a glass material and a photon mapping path tracer. I can now produce approximately correct caustics with much less number of samples thanks to the photon map. Possible improvements include a more efficient data structure storing photons, and a better estimation functions preserving details in caustics. The cover image is rendered with $2048$ samples in $1000 \\times 1000$ resolution, which I think fully demonstrate the reflection, refraction and caustics effects I achieved. Reference Global Illumination using Photon Maps A Practical Guide to Global Illumination using Photon Maps Fundamentals of Rendering - Reflectance Functions Photon Mapping - Zack Waters 光子映射总结（1/4）：基本全局光子映射（Basic Photon Mapping） Easter Egg :-)This is a variant of the cover image rendered with same configuration. Thanks for reading!","link":"/TonyZYT2000.github.io.archive/en/CSE168-Final-Project/"},{"title":"CSE168 Project 1","text":"Ray Tracer with Phong Shading $$I = A + E + \\sum_{i} V_i\\frac{L_i}{c_0 + c_1r + c_2 r^2}\\left(K_d \\cdot max(n \\cdot l, 0) + K_s\\cdot max(n \\cdot h, 0)^s\\right)$$ Long time no see! For the past quarters, I took some fantastic courses in computer graphics, and discovered my interests in this field. Starting from this post, I will document my homework projects from CSE168 rendering course. Although I did not add any extra features for project 1, I plan to learn more by implementing extra features in later projects. Future posts will include documentations of extra features so that TAs could evaluate my work easily. The following image is rendered by a simple ray tracer. We’ll see how it evolves in the following weeks. j","link":"/TonyZYT2000.github.io.archive/en/CSE168-Project1/"},{"title":"CSE168 Project 2","text":"Direct Lighting with Monte Carlo Integration $$L_d(x, \\omega_o) \\approx L_i \\frac{A}{N} \\cdot\\sum_{k = 1}^{N}f\\left(x, \\omega_i(k), \\omega_o\\right)\\frac{\\cos \\theta_i \\cdot \\cos \\theta_o}{\\mid x - x_k’\\mid^2}V(x, x_k’)$$ IntroductionIn this project, I implemented direct light sampling of area light sources with Monte Carlo integration. The equation above represents the Monte Carlo integration of one area light source $L_i$ with $N$ samples. The final color of one pixel is essentially the sum of emission and direct lightings from all area light sources. Below are some output results. Click them to view larger images: Sphere Cornell Box Dragon Variance and NoiseNotice that noises can be found in the image, especially in the shadows. This is caused by the variance of Monte Carlo integration with finite number of samples. As the number of samples increases, the variance could decrease, but just at a linear rate. So, apart from randomly sampling the area light, a stratified sampling method was implemented to further reduce the variance. Stratified sampling method divides the area light into little grids, and samples from each grid. This process essentially makes samples more uniformly distributed than purely random distribution, and thus reduce the noise. 9 Random Sampling 3x3 Stratified Random Sampling More Variance Reduction MethodsAfter writing the stratified sampling integrator, I also implemented two extra variance reduction methods. Biased Center SamplingThe first one is to sample area light source at the center, and only sample randomly for visibility. This method is biased, but could completely remove noise in area without shadows. Codes are written in OptiXRenderer/DIrectBiased.cu. The image below sampled the area light source only at the center. So, although there is no noise, the image is totally wrong and has visible bias. The bias can be reduced in combination with stratified sampling such that we are not just sampling the single center of the area light, but several centers of the little grids in the area light. The image below sampled centers from 3x3 stratified grids of the area light source. It appears very similar to the physically correct result, and there is no noise at all. 3x3 Stratified Biased Sampling 3x3 Stratified Random Sampling Bias: $-0.2$, Variance: 0.3 Bias: 0.0, Variance: 3.8 As the number of grids increases, the bias can be further reduced. Below are more comparisons between this biased method sampling at center, and random sampling method. 5x5 Stratified Biased Sampling 5x5 Stratified Random Sampling Bias: -0.1, Variance: 1.4 Bias: 0.0, Variance: 2.8 Bias: 0.0, Variance: 0.5 Bias: 0.0, Variance: 1.0 Bias: 0.0, Variance: 0.5 Bias: 0.0, Variance: 0.8 In the sphere scene and Cornell box scene, there are clearly less noise in areas without shadows. For example, the upper hemisphere is very smooth because there is no obstacle blocking it. However, this effect is not obvious in the dragon scene. In addition, because of this center sampling pattern is fixed across all pixels, some binding effects can be spotted in the left lower corner of the dragon scene. Overall, this method can significantly reduce noise in areas without shadows, but has no improvement on shadows, and may even cause banding issues in the shadows. Low Discrepancy Sequence SamplingWhile we enforce uniform distribution in stratified random sampling by cutting area light into grids, another approach is to directly generate numbers that area uniformly distributed. Formally, such numbers form a low discrepancy sequence, and Monte Carlo integration using low discrepancy sequence is called quasi Monte Carlo method. Below I will just refer this method as quasi random method for simplicity. I searched for some low discrepancy sequences, and initially want to implement a Sobol sequence generator to use in the integrator. However, I failed to comprehend all the math behind it, and in the end chose a simpler one called Van der Corput sequence. Since the integrator need to sample at a 2D area light source, I used two Van der Corput sequences, and this multidimensional generalization is called Halton sequence. Codes are written in OptiXRenderer/lowdis.h and OptiXRenderer/DirectQuasi.cu. 9 Quasi Random Sampling 9 Random Sampling 3x3 Stratified Random Sampling Bias: 0.0, Variance: 3.7 Bias: 0.0, Variance: 28.9 Bias: 0.0, Variance: 3.8 Images above show how the quasi random method reduced the variance without using stratification. It does not introduce any bias, and the final result is comparable with stratified random sampling with same number of samples. This is expected as both methods enforce better uniform distribution. Below are more comparisons between quasi random method and stratified random method. 25 Quasi Random Sampling 5x5 Stratified Random Sampling Bias: 0.0, Variance: 3.5 Bias: 0.0, Variance: 2.8 Bias: 0.0, Variance: 1.1 Bias: 0.0, Variance: 1.0 Bias: 0.0, Variance: 0.9 Bias: 0.0, Variance: 0.8 Overall, this quasi random method produces similar results as stratified random method. There is no bias or binding effect, which makes it an equivalently good alternative to the stratified random method. Because of my naïve implementation of sequence generator, the quasi random method does not have obvious performance difference from stratified random method. But since no stratification is required in quasi random method, it may gain better performance from getting rid of iteration. The Sobol sequence included in the reference could be a good choice since it replaces a lot of arithmetics with efficient bit operations. Full Reports of Submissions to UCSD OnlineHere are the link to reports generated on UCSD Online: Random and Stratified Random Sampling: https://raviucsdgroup.s3.amazonaws.com/homework2/7ace409cf67531f2bb8a6fcfb7d7dfad/20210423042234/index.html Biased Center Sampling: https://raviucsdgroup.s3.amazonaws.com/homework2/7ace409cf67531f2bb8a6fcfb7d7dfad/20210423041831/index.html Low Discrepancy Sequence Sampling: https://raviucsdgroup.s3.amazonaws.com/homework2/7ace409cf67531f2bb8a6fcfb7d7dfad/20210423042033/index.html Reference An introduction to quasi-random numbers 低差异序列（一）- 常见序列的定义及性质 低差异序列（二）- 高效实现以及应用 Notes on generating Sobol’ Sequences Sobol sequence generator","link":"/TonyZYT2000.github.io.archive/en/CSE168-Project2/"},{"title":"Hello World Again","text":"Hello World Again :-) This is ZYT’s blog. I say “Hello World Again” because I setup this Hexo blog in spring break earlier, but stopped due to school work and my laziness. Last week, after the summer session, I finally finished a tiny project started in the spring break. What coming next is this blog. This time, I switch to theme Minos. I read Hexo documentations in depth, add multi-language support, and setup a GitHub-based image hosting service. I hope all these works can encourage me to keep writing blogs. I will start with sharing some photographs taken last summer vacation. Then, I will share my notes on Scoop, Vim, gcov, and other technical topics. A journey of a thousand miles begins with single step.","link":"/TonyZYT2000.github.io.archive/en/Hello-World-Again/"},{"title":"VIS60 Homework 3","text":"Black’s Beach, 07/29/2019 This is the third homework of VIS60 last summer. This homework focused on printing, and asked us to print one large photo with one small photo. I chose to took a panorama of the La Jolla Cove from the Black’s beach next to school,. The printing experience was fascinating. I processed and printed the full size 15600 × 5126 photo onto an A3 size photo paper. The rich details and vivid color made me feel I can teleport into the world inside the photo. As for the small photo, I decided to post the large one on social media, and took a picture of my phone. It is then printed on an A4 size paper with large borders to create the dramatic comparison between the large photo and the small photo. I hope such comparison can evoke some thinking on social media, cyber space and the internet. In summary, I not only practiced taking processing panoramas, but also learned how to print large photos. Although there were some defects caused by combining different photos, the buildings were clean and sharp. It was really rewarding when I printed it out and held it in my hand. By the way, this is the last homework of the course. I took photos on birds for the final project, so I will post them for birding blogs. Thanks for viewing my works in the fun photography course VIS60!","link":"/TonyZYT2000.github.io.archive/en/VIS60-Project3/"},{"title":"VIS60 Homework 1","text":"La Jolla Cove, 07/13/2019 Update my blog with photography homework from VIS60 last summer. Done some modification and host these images in a GitHub repo. If above four images appear correctly, then the hosting system is working as I expected. Click them to see larger images! Short artist statement: This homework asked us to explore a small area. I chose the streets around La Jolla Cove and took pictures of street scenes. Images were about buildings, plants and vehicles. The theme is discovering unnoticed landscapes aside usual street. I hope to present these common landscapes in an interesting way. What I seek is a simple composition with geometric harmony. That’s it! Looking forward to post the second homework.","link":"/TonyZYT2000.github.io.archive/en/VIS60-Project1/"},{"title":"VIS60 Homework 2","text":"Scripps Memorial Pier, 07/21/2019 This is the second homework of VIS60 last summer. This homework asked us to choose a single object and demonstrate that object in different ways. I chose the Scripps Pier located at La Jolla. Scripps Institution of Oceanography takes samples of the ocean from this pier for scientific research. Artist statement: I took five pictures of the pier at different times from different angles. I want to demonstrate the look of the pier under changing point of views and light resources. The adding of background elements, like the surfing people and seagulls, further illustrated the flow of time around the pier. The most fun part of this homework was walking around and finding new positions to take pictures. I also enjoyed the light during sunset, which added beautiful tones to the pier. In short, this homework is very enjoyable and I learnt a lot from it. Sticking to a single object is a great way to train myself. Also, the scenery of Scripps Beach is very beautiful. It is a perfect place for everyone to relax!","link":"/TonyZYT2000.github.io.archive/en/VIS60-Project2/"}],"tags":[{"name":"CSE168","slug":"CSE168","link":"/TonyZYT2000.github.io.archive/en/tags/CSE168/"},{"name":"BRDF","slug":"BRDF","link":"/TonyZYT2000.github.io.archive/en/tags/BRDF/"},{"name":"Rendering","slug":"Rendering","link":"/TonyZYT2000.github.io.archive/en/tags/Rendering/"},{"name":"Ray Tracing","slug":"Ray-Tracing","link":"/TonyZYT2000.github.io.archive/en/tags/Ray-Tracing/"},{"name":"Photon Mapping","slug":"Photon-Mapping","link":"/TonyZYT2000.github.io.archive/en/tags/Photon-Mapping/"},{"name":"Monte Carlo Integration","slug":"Monte-Carlo-Integration","link":"/TonyZYT2000.github.io.archive/en/tags/Monte-Carlo-Integration/"},{"name":"Emotions","slug":"Emotions","link":"/TonyZYT2000.github.io.archive/en/tags/Emotions/"},{"name":"Landscape","slug":"Landscape","link":"/TonyZYT2000.github.io.archive/en/tags/Landscape/"},{"name":"Seaside","slug":"Seaside","link":"/TonyZYT2000.github.io.archive/en/tags/Seaside/"},{"name":"Streetscape","slug":"Streetscape","link":"/TonyZYT2000.github.io.archive/en/tags/Streetscape/"}],"categories":[{"name":"Computer Science","slug":"Computer-Science","link":"/TonyZYT2000.github.io.archive/en/categories/Computer-Science/"},{"name":"Ideas","slug":"Ideas","link":"/TonyZYT2000.github.io.archive/en/categories/Ideas/"},{"name":"Photography","slug":"Photography","link":"/TonyZYT2000.github.io.archive/en/categories/Photography/"},{"name":"Computer Graphics","slug":"Computer-Science/Computer-Graphics","link":"/TonyZYT2000.github.io.archive/en/categories/Computer-Science/Computer-Graphics/"}]}